---
layout: article
title: Winograd Convolution in Detail
tags: Convolution_Neural_Network
---

Winograd convolution is a fast and efficient convolution algorithm that uses a set of precomputed matrices to perform the convolution. It reduces the number of arithmetic operations required compared to other methods by exploiting the properties of polynomial evaluation. This algorithm is particularly useful for small filter sizes and can be implemented efficiently on parallel hardware architectures.

##### Definition of $F(m, r) $ and its 2D case

In general, the notation $F(m, r)$ represents a one-dimensional convolution operation with a filter size of $r$ and an output size of $m$. In two-dimensional space, this notation is extended to $F(m\times n, r\times s)$, which refers to a convolution operation with a filter size of $r\times s$ and an output size of $m\times n$.

##### Standard Convolution Algorithm Complexity

To illustrate the concept of convolution, consider the example of $F(2, 2)$, where the input, output, and kernel are denoted by $x = (x_0, x_1, x_2)$, $y = (y_0, y_1)$, and $g = (g_0, g_1)$, respectively. The computation is performed as follows:

$$
\begin{aligned}
y_0 = g_0 x_0 + g_1 x_1 \\ 
y_1 = g_0 x_1 + g_1 x_2
\end{aligned}\qquad\qquad (1)
$$

This operation involves four multiplications and two additions. 

##### Fast Algorithm of $F(2, 2)$

Rewrite Eq.(1) as 

$$
  \begin{aligned}
  y_0 &= g_0x_0 + g_1 x_1\\
  &=g_0x_0 - g_0x_1 + g_0x_1+ g_1x_1 \\
  &= g_0(x_0 - x_1) + (g_0 + g_1)x_1
  \end{aligned}\qquad\qquad (2)
$$

$$
\begin{aligned}
y_1 &= g_0x_1 + g_1x_2 \\
&= g_0x_1 +  g_1x_1 -  g_1x_1 + g_1x_2 \\
&= (g_0 + g_1)x_1 - g_1 (x_1 - x_2)
\end{aligned}\qquad\qquad (3)
$$

and let $m_1 = (x_0 - x_1)g_0, m_2 = (g_0 + g_1)x_1, m_3 = g_1(x_1 - x_2)$, the above formula becomes 

$$
\begin{aligned}
y_0 = m_1 + m_2\\
y_1 = m_2 - m_3
\end{aligned}\qquad\qquad (4)
$$

It is evident that the number of multiplications is reduced from four to three as a result of this transformation. However, the number of additions increases to five. The primary advancement of this type of transformation is the reduction of the number of multiplications at the cost of increasing the number of additions.

##### Fast Algorithm of $F(2, 3)$

It's not hard to extend the above method to $F(2, 3)$, which results in

$$
    \left[
    \begin{aligned}
    y_0\\y_1
    \end{aligned}
    \right]=\left[
    \begin{aligned}
    x_0 \quad x_1 \quad x_2\\
    x_1 \quad x_2 \quad x_3
    \end{aligned}
    \right]\cdot
  \left[
    \begin{aligned}
    g_0\\g_1\\g_2
    \end{aligned}
    \right] = 
    \left[
      \begin{aligned}
      m_1+ m_2 + m_3\\
      m_2 - m_3 - m_4
      \end{aligned}
    \right]\qquad\qquad (5)
  $$

where 

$$
\begin{aligned}
m_1 &= (x_0 - x_2)g_0\\
m_2 &= (x_1 + x_2)\frac{g_0 + g_1 + g_2}{2}\\
m_3 &= (x_2 - x_1)\frac{g_0 - g_1 + g_2}{2}\\
m_4 &= (x_1 - x_3)g_2
\end{aligned}
$$

And it can be reformulated as matrix form:

$$
y = x\star g= A^\top \left[(L g)\odot (B^\top x)\right] \qquad\qquad (6)
$$

Well, it seems not that apparently. Let's dive in deep to see what happend here. Let's start from

$$
y = \left[
      \begin{aligned}
      m_1 + m_2 + m_3 \\
      m_2 - m_3 - m_4
      \end{aligned}
    \right] =
    \left[\begin{aligned}
&1 \quad &1 \quad\quad &1 \quad &0\\
&0 \quad &1 \quad -&1 \quad &-1
\end{aligned}\right]
\left[
      \begin{aligned}
      m_1\\
      m_2\\
      m_3\\
      m_4
      \end{aligned}
    \right] = A^\top M \qquad\qquad (7)
$$

where 

$$
A^\top = \left[\begin{aligned}
&1 \quad &1 \quad\quad &1 \quad &0\\
&0 \quad &1 \quad -&1 \quad &-1
\end{aligned}\right]
$$

and 

$$
M = \left[
      \begin{aligned}
      x_0 - x_2\\
      x_1 + x_2\\
      x_2 - x_1\\
      x_1 - x_3
      \end{aligned}
    \right]
    \odot 
    \left[
      \begin{aligned}
      g_0\\
      \frac{g_0 +g_1 +g_2}{2}\\
      \frac{g_0 - g_1 +g_2}{2}\\
      g_2
      \end{aligned}
    \right]
$$

consider that $x_0 - x_2 = x_0 + 0 x_1 - x_2 + 0 x_3$, and apply it to all element, we then have

$$
\left[
  \begin{aligned}
  x_0 - x_2\\
  x_1 + x_2\\
  x_2 - x_1\\
  x_1 - x_3
  \end{aligned}
\right] = \left[
  \begin{aligned}
  1\quad& 0 & -1 &\quad 0\\
  0\quad& 1 & 1 &\quad 0\\
  0\quad& -1& 1 &\quad 0\\
  0\quad& 1 & 0 &\quad -1
  \end{aligned}
\right] \left[
  \begin{aligned}
  x_0\\
  x_1\\
  x_2\\
  x_3
  \end{aligned}
\right] = B^\top x
$$

$$
\left[
  \begin{aligned}
  g_0\\
  \frac{g_0 + g_1 + g_2}{2}\\
  \frac{g_0 - g_1 + g_2}{2}\\
  g_2
  \end{aligned}
\right] = \left[
  \begin{aligned}
  1\quad& 0 & 0\\
  \frac{1}{2}\quad& \frac{1}{2} & \frac{1}{2}\\
  \frac{1}{2}\quad& -\frac{1}{2} & \frac{1}{2}\\
  0 \quad& 0 &\quad 1
  \end{aligned}
\right] \left[
  \begin{aligned}
  g_0\\
  g_1\\
  g_2
  \end{aligned}
\right] = L g
$$

Put the above questions all together, we will get the matrix form $F(2, 3)$:

$$
y = x\star g= A^\top \left[(L g)\odot (B^\top x)\right] \qquad\qquad (9)
$$

##### Towards 2D case

To obtain a 2D convolution, a 1D convolution is nested with itself. For instance, consider the example of $F(2\times 2, 3\times 3)$. Its 1D counterpart is $F(2, 3)$, as previously described in the preceding section:

$$
y = F(2, 3) = x\star g = A^\top \left[(L g)\odot (B^\top x)\right]
$$

Here, we denote the input 2D array as $X$, where

$$
X = \left[
\begin{aligned}
X_0\\X_1\\X_2\\X_3
\end{aligned}
\right]
$$

and kernel as $G$

$$
G = \left[
\begin{aligned}
G_0\\G_1\\G_2
\end{aligned}
\right]
$$

Then the output $Y$ can be written as

$$
\begin{aligned}
Y &= X \star G\\
&= \left[
      \begin{aligned}
      M_1+ M_2 + M_3\\
      M_2 - M_3 - M_4
      \end{aligned}
    \right]\\
  &=A^\top \left[
      \begin{aligned}
      M_1\\ M_2\\ M_3\\ M_4
      \end{aligned}
    \right]
\end{aligned} \qquad\qquad (10)
$$

where 

$$
\begin{aligned}
M_1 &= (X_0 - X_2)\star G_0\\
M_2 &= (X_1 + X_2)\star\frac{G_0 + G_1 + G_2}{2}\\
M_3 &= (X_2 - X_1)\star\frac{G_0 - G_1 + G_2}{2}\\
M_4 &= (X_1 - X_3)\star G_2
\end{aligned}
$$

For simplicity, lets denote

$$
\begin{aligned}
R_1 = X_0 - X_2,&\quad S_1 = G_0\\
R_2 = X_1 + X_2,&\quad S_2 = \frac{G_0+G_1+G_2}{2}\\
R_3 = X_2 - X_1,&\quad S_3 = \frac{G_0 - G_1 + G_2}{2} \\
R_4 = X_1 - X_3,&\quad S_4 = G_2
\end{aligned}
$$

Then we have

$$
M_i = R_i \star S_i , \quad i \in [1,2,3,4] \qquad\qquad (11)
$$

Consider that the $R_i, S_i$ are all 1D row vector, then the matrix form of the above convolution is 

$$
R_i \star S_i = [(S_i L^\top) \odot (R_i B)] A \qquad\qquad (12)
$$

Substitute formula (12) to (11), and then to (10), we get 

$$
Y = A^\top \left[
      \begin{aligned}
      (S_1 L^\top) \odot (R_1 B)\\
      (S_2 L^\top) \odot (R_2 B)\\
      (S_3 L^\top) \odot (R_3 B)\\
      (S_4 L^\top) \odot (R_4 B)
      \end{aligned}
    \right] A  =
    A^\top \left(\left[
      \begin{aligned}
      S_1\\S_2\\S_3\\S_4
      \end{aligned}
    \right] L^\top \odot
    \left[
      \begin{aligned}
      R_1\\R_2\\R_3\\R_4
      \end{aligned}
    \right] B \right) A \qquad\qquad (13)
$$

And it's easy to derivate that 

$$
\left[
      \begin{aligned}
      R_1\\
      R_2\\
      R_3\\
      R_4
      \end{aligned}
    \right] = B^\top X, \quad \left[
      \begin{aligned}
      S_1\\
      S_2\\
      S_3\\
      S_4
      \end{aligned}
    \right] = L G 
$$

Substitude it to formula (13), we finally get

$$
Y = A^\top \left[(L G L^\top)\odot (B^\top X B)\right] A = A^\top (U\odot V)A \qquad \qquad (14)
$$

where $U = L G L^\top$ and $V = B^\top X B$, and $U, V$ can be seen as some transformation of $G, X$, respectively.

##### Convolution Layer

A Convolution Layer uses a bank of filters and a mini-batch of images as inputs. The number of filters is defined as $K$, the image channel as $C$, and the size of the image and filter are $H_i\times W_i$ and $H_f\times W_f$, respectively. The following figure illustrates the computation.:

![](/en/resources/2023-04-06-winograd/winograd_conv-layer.png)

it is also can be written as:

$$
Y_{i, k, x, y} = \sum_{c=1}^C \sum_{u=1}^{H_f} \sum_{v=1}^{W_f}  X_{i, c, u+x, s+v} G_{k, c, u, v}\qquad\qquad (15)
$$

or more compactly in the form of entire image/filter convolution:

$$
Y_{i, k} = \sum_{c = 1}^C X_{i, c} \star G_{k, c} \qquad\qquad (16)
$$

where $X_{i, c}$ is the i-th image in minibatch and c-th channel of the image.

##### Tile-based Decomposition of Convolution

A standard convolution can be decomposed into a series small image-tile convolutions (i.e. $F(m\times m, r\times r)$) as the following figure shows:

![](/en/resources/2023-04-06-winograd/winograd_conv-layer-tiled.png)

where $X_{i, c, \hat{x}, \hat{y}}$ is the image-tile at coordinate $(\hat{x}, \hat{y})$ on image $X_{i, c}$ with shape $\alpha = m + r - 1$, and suppose the tile number on each image is $\hat{H}\times \hat{W}$. With tile decomposition, the convolution on the tile is

$$
Y_{i, k, \hat{x}, \hat{y}} = \sum_{c=1}^C X_{i, c, \hat{x}, \hat{y}} \star G_{k, c} \qquad\qquad (17)
$$

following the formula (14), we can write the above formula as:

$$
\begin{aligned}
Y_{i, k, \hat{x}, \hat{y}} &= \sum_{c=1}^C A^\top (U_{k, c}\odot V_{i, c, \hat{x}, \hat{y}}) A
&= A^\top \left(\sum_{c=1}^C U_{k, c}\odot V_{i, c, \hat{x}, \hat{y}}\right) A
\end{aligned} \qquad\qquad (18)
$$

let $M_{k,i,\hat{x}, \hat{y}}$ be the middle sum term

$$
M_{k,i,\hat{x}, \hat{y}} = \sum_{c=1}^C U_{k,c} \odot V_{i, c, \hat{x}, \hat{y}} \qquad\qquad (19)
$$

and the figure bellow shows the raw computation of $M_{k,i,\hat{x}, \hat{y}}$.

![](/en/resources/2023-04-06-winograd/winograd_conv-layer-tiled_inner-sum.png)

However, $M$ and $V$ can be simplified by collapsing the image/tile coordinates $(i, \hat{x}, \hat{y})$ down to one dimension of size $N \times \hat{H} \times \hat{W}$. Denote $b$ as the index of the collapsed dimension, then:

$$
M_{k, b} = \sum_{c=1}^C U_{k, c} \odot V_{c, b} \qquad\qquad (20)
$$

and

$$
Y_{k, b} = A^\top M_{k, b} A \qquad\qquad (21)
$$

Label the element coordinate of $M, U, V$ as $(\xi, \eta)$, yielding:

$$
M_{k, b}^{\xi, \eta} = \sum_{c=1}^C U_{k, c}^{\xi, \eta} V_{c, b}^{\xi, \eta} \qquad\qquad (22)
$$

Fix $\xi,\eta$, and iterate over all $c, b$ to take the elements of $V$ to form a matrix $V^{\xi,\eta}$ of shape $C\times N\hat{H}\hat{W}$ as the following figure shows:

![](/en/resources/2023-04-06-winograd/winograd_v-xi-eta.png)


And apply the same procedure to generate $U^{\xi, \eta}$ of shape $K\times C$, Since $U^{\xi, \eta}, V^{\xi, \eta}$ are both matrix, then formula (22) is essentially a matrix multiplication:

$$
M^{\xi, \eta} = U^{\xi, \eta} V^{\xi, \eta} \qquad\qquad (23)
$$

It's easy to gather to $M_{k, b}$ for every $k, b$ from $M$ which is a 4-d tensor obtained by iterating over $\xi, \eta$ using formula (23). And finally, we can get the output $Y_{k,b}$ by applying formula (21).

##### Filter Transformation

Filter transformation is the procedure to generate $U$ using the method in previous section. 

$$
U_{k, c} = L G_{k, c} L^\top, \quad shape: 4\times 4
$$

$$
V_{i, c, \hat{x}, \hat{y}} = B^\top X_{i, c, \hat{x}, \hat{y}} B, \quad shape: 4\times 4
$$

$$
M^{\tau} = U^{\tau} V^{\tau}
$$

$$
\left[
\begin{aligned}
M^1\\M^2\\.\\.\\M^{\Tau}
\end{aligned}
\right]
$$

##### Conclusion

In this post, we derivate the winograd convolution algorithm from the scratch which is  omitted by the original paper[1], hope it is helpful to understand the winograd convolution.

##### Reference

[1]. [Fast Algorithms for Convolutional Neural Networks](https://arxiv.org/abs/1509.09308)
